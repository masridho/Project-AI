{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY7rW_XVTI3t"
      },
      "outputs": [],
      "source": [
        "#Melakukan import library yang dibutuhkan\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import requests\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, GlobalMaxPool2D, concatenate\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import random\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import Model\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import syntax augmentor dan opendataset\n",
        "!pip install Augmentor --quiet\n",
        "import Augmentor\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "!pip install opendatasets\n",
        "import opendatasets as od"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIraEChWTaNA",
        "outputId": "5d4dcaa4-8ea8-4c1e-8678-701c3e702e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2022.9.24)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (6.1.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/jonathanrjpereira/rice-disease\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxSBAFTBTdXZ",
        "outputId": "5f0f815b-825e-44e3-f63f-5d7f5cf16880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: muhammadyusufridho\n",
            "Your Kaggle Key: ··········\n",
            "Downloading rice-disease.zip to ./rice-disease\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.55G/4.55G [02:30<00:00, 32.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preparation\n"
      ],
      "metadata": {
        "id": "LKshtVabUdwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#membuat label source path, train dan valid\n",
        "source_path = 'rice-disease/Rice_All'\n",
        "#membuat label source train\n",
        "source_train = os.path.join(source_path, 'train')\n",
        "#membuat label source valid\n",
        "source_test = os.path.join(source_path, 'valid')"
      ],
      "metadata": {
        "id": "mJsRFJFAUfig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#membuat training path dan testing path sekaligus membuat folder nya sekaligus\n",
        "training_path = 'rice-disease/train'\n",
        "testing_path = 'rice-disease/test'\n",
        "\n",
        "#membuat folder baru bernama data\n",
        "\n",
        "try:\n",
        "  os.mkdir(testing_path)\n",
        "except:\n",
        "  print('path exist')\n",
        "\n",
        "try:\n",
        "  os.mkdir(training_path)\n",
        "except:\n",
        "  print('path exist')\n",
        "\n",
        "list_class = os.listdir(training_path)\n",
        "print(list_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5oqIMUFVTH3",
        "outputId": "f1750137-feb9-4e79-9f02-6b7e7c6550f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#kita melihat kategori apa saja yang ada didalam data tersebut\n",
        "categories = os.listdir(source_path)\n",
        "categories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9X9xjIxVo57",
        "outputId": "f1215bdd-2e2c-4143-dcf9-fa188dd6e0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Healthy', 'LeafBlast', 'BrownSpot', 'Hispa']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# membuat folder di data training path maupun testing path sesuai dengan list kategori yang sudah dibuat\n",
        "for category in categories:\n",
        "  try:\n",
        "    os.mkdir(os.path.join(training_path, category))\n",
        "    os.mkdir(os.path.join(testing_path, category))\n",
        "  except:\n",
        "    print('path exist!')"
      ],
      "metadata": {
        "id": "O73DBleZWBLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#menentukan label kelas dari masing masing kategori yang sudah di urutkan\n",
        "#SET LABEL CLASS\n",
        "dic_labels = {}\n",
        "categories.sort()\n",
        "for index, category in enumerate(categories):\n",
        "  dic_labels[category] = str(index)\n",
        "dic_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIsFDUVmWUN2",
        "outputId": "dd34920b-9778-46e8-e4bd-13dc359f9cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BrownSpot': '0', 'Healthy': '1', 'Hispa': '2', 'LeafBlast': '3'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# melihat lagi jumlah isinya ?\n",
        "dic_class = {'class' : [], 'total_img' : []}\n",
        "for category in categories :\n",
        "  total_img = os.listdir(os.path.join(testing_path, category))\n",
        "  dic_class['class'].append(category)\n",
        "  dic_class['total_img'].append(len(total_img))\n",
        "dic_class\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfns0OEdXBI3",
        "outputId": "a8aa1199-863b-4766-debf-6b8a73381c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class': ['BrownSpot', 'Healthy', 'Hispa', 'LeafBlast'],\n",
              " 'total_img': [523, 523, 523, 523]}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mindahin gambar folder train\n",
        "ROOT_DIR = os.path.abspath(os.curdir)\n",
        "os.chdir(ROOT_DIR)\n",
        "for i in range(len(dic_class['class'])):\n",
        "  os.chdir(ROOT_DIR)\n",
        "  folder_path = os.path.join(source_path, dic_class['class'][i] )\n",
        "  list_img = os.listdir(folder_path)\n",
        "  for img in list_img:\n",
        "    if 'JPG' in img or 'jpg' in img :\n",
        "      file_path = os.path.join(folder_path, img)\n",
        "      image = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
        "      resized = cv2.resize(image, (224, 224), interpolation = cv2.INTER_AREA)\n",
        "      os.chdir(os.path.join(training_path, dic_class['class'][i]))\n",
        "      cv2.imwrite(img, resized)\n",
        "      os.chdir(ROOT_DIR)\n",
        "os.chdir(ROOT_DIR)"
      ],
      "metadata": {
        "id": "r-l_OuHoXISl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mindahin gambar ke folder testing\n",
        "ROOT_DIR = os.path.abspath(os.curdir)\n",
        "os.chdir(ROOT_DIR)\n",
        "for i in range(len(dic_class['class'])):\n",
        "  os.chdir(ROOT_DIR)\n",
        "  folder_path = os.path.join(source_path, dic_class['class'][i] )\n",
        "  list_img = os.listdir(folder_path)\n",
        "  for img in list_img:\n",
        "    if 'JPG' in img or 'jpg' in img :\n",
        "      file_path = os.path.join(folder_path, img)\n",
        "      image = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
        "      resized = cv2.resize(image, (224, 224), interpolation = cv2.INTER_AREA)\n",
        "      os.chdir(os.path.join(testing_path, dic_class['class'][i]))\n",
        "      cv2.imwrite(img, resized)\n",
        "      os.chdir(ROOT_DIR)\n",
        "os.chdir(ROOT_DIR)"
      ],
      "metadata": {
        "id": "Jssiw-_IXQeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#augmentation\n",
        "def perbanyak_(ini, sebanyak_ini):\n",
        "  source_dir = ini\n",
        "  output_dir = \".\"\n",
        "  p = Augmentor.Pipeline(source_directory=source_dir, output_directory=output_dir)\n",
        "  p.rotate(probability=1, max_left_rotation=5, max_right_rotation=5)\n",
        "  p.skew_left_right(probability=0.7, magnitude=0.1)\n",
        "\n",
        "  p.sample(sebanyak_ini)\n",
        "\n",
        "\n",
        "jumlah_data = 800\n",
        "for category in os.listdir(os.path.join(training_path)):\n",
        "  cof = os.path.join(training_path,category)\n",
        "  numb = len(os.listdir(cof))\n",
        "  \n",
        "  perbanyak_(cof, jumlah_data - numb)"
      ],
      "metadata": {
        "id": "bnhhDOLbaSsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#membagi x test dan y test\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "list_category = os.listdir(testing_path)\n",
        "list_category.sort()\n",
        "for category in list_category :\n",
        "  list_image = os.listdir(os.path.join(testing_path, category))\n",
        "  for image in list_image :\n",
        "    img = cv2.imread(os.path.join(testing_path, category, image), cv2.IMREAD_UNCHANGED)\n",
        "    resized = cv2.resize(img, (224, 224), interpolation = cv2.INTER_AREA)\n",
        "    img = np.array(resized)\n",
        "    X_test.append(img)\n",
        "    y_test.append(dic_labels[category])"
      ],
      "metadata": {
        "id": "SsIaRxHiapXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array(X_test)\n",
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e67_K6vea3sv",
        "outputId": "61d34f37-4f70-480d-9e16-62ce40e240bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2092, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.keras.utils.image_dataset_from_directory(training_path,  image_size=(224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooYlsiXaa8Am",
        "outputId": "0764d0dc-a134-4cfc-f2ab-fc14e6225af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3200 files belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting Data\n",
        "train_size = int(len(data)*.7)\n",
        "val_size = len(data)-train_size"
      ],
      "metadata": {
        "id": "oERkgbJda_uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#membuat label train dan val\n",
        "train = data.take(train_size)\n",
        "val = data.skip(train_size).take(val_size)"
      ],
      "metadata": {
        "id": "23pzOsGNbCYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model VGG19"
      ],
      "metadata": {
        "id": "KcsKoogTbDrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = VGG19(include_top = False, weights='imagenet', input_shape=(224, 224, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNRqchlFbG9V",
        "outputId": "1fa76e18-52d2-45e3-d88f-b4d558724722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#memangkas 5 bagian bawah dari vgg19 sesuai jurnal\n",
        "base_model = tf.keras.models.Sequential(base_model.layers[:-5])"
      ],
      "metadata": {
        "id": "FQXTm21ZbJQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#membuat inception module\n",
        "def inception_module(layer_in, f1, f2, f3):\n",
        "\t# 1x1 conv\n",
        "\tconv1 = Conv2D(f1, (1,1), padding='same', activation='swish')(layer_in)\n",
        "\t# 3x3 conv\n",
        "\tconv3 = Conv2D(f2, (3,3), padding='same', activation='swish')(layer_in)\n",
        "\t# 5x5 conv\n",
        "\tconv5 = Conv2D(f3, (5,5), padding='same', activation='swish')(layer_in)\n",
        "\t# 3x3 max pooling\n",
        "\tpool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
        "\t# concatenate filters, assumes filters/channels last\n",
        "\tlayer_out = concatenate([conv1, conv3, conv5], axis=-1)\n",
        "\treturn layer_out\n",
        "  "
      ],
      "metadata": {
        "id": "wl4hF0hvbLjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_new_layer = base_model.output\n",
        "base_model.output.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5saihq1lbOWU",
        "outputId": "01aab52a-6fee-49ab-af54-784e709896c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 14, 14, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPrpXlyHbRKm",
        "outputId": "57a5513b-9ffb-4ad5-e3c4-688c9571ec2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, 224, 224, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#menambahkan layer baru ke model vgg19\n",
        "output_new_layer = BatchNormalization(input_shape=(7,7, 512))(output_new_layer)\n",
        "output_new_layer = inception_module(output_new_layer, 64, 64, 128)\n",
        "#output_new_layer = inception_module(output_new_layer, 64, 64, 128)\n",
        "output_new_layer = GlobalMaxPool2D()(output_new_layer)\n",
        "predictions = Dense(len(list_class), activation='softmax', name='pred') (output_new_layer)\n",
        "new_model = Model(inputs=base_model.input, outputs = predictions)\n"
      ],
      "metadata": {
        "id": "HrZSaRbpbTA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#melakukan frezee sd bloc4_pool\n",
        "for layer in new_model.layers:\n",
        "  layer.trainable = False\n",
        "  if 'block4_pool' in layer.name:\n",
        "    break"
      ],
      "metadata": {
        "id": "pwUuyzebbVBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#persiapan melakukan compile dengan optimizer, loss dan metrics\n",
        "new_model.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7VVpcbyZbYBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = 'logs'\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "metadata": {
        "id": "Y_aak0l8baEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EarlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "cA_yiR4bbcEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#melakukan training dengan label hist\n",
        "hist = new_model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "70zBxErWbd4g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}